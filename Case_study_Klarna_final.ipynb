{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import tensorflow.keras as keras\n",
    "import math\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import metrics, regularizers, optimizers\n",
    "\n",
    "\n",
    "#import sklearn.metrics \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# The size of the plots.\n",
    "mpl.rcParams['figure.figsize'] = (7,7)\n",
    "\n",
    "# To have the plots inside the notebook \"inlin\" should be True. \n",
    "# If \"inlin\" = False, then plots will pop out of the notebook\n",
    "inlin = True # True/False\n",
    "if inlin:\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib \n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our model framework and creating functions for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipline(inp_dim,\n",
    "            n_nod,\n",
    "            act_fun = 'relu',\n",
    "            out_act_fun = 'sigmoid',\n",
    "            opt_method = 'Adam',\n",
    "            cost_fun = 'binary_crossentropy',\n",
    "            lr_rate = 0.01, \n",
    "            lambd = 0.0, \n",
    "            num_out = None):\n",
    "    \n",
    "    lays = [inp_dim] + n_nod\n",
    "    \n",
    "    main_input = Input(shape=(inp_dim,), dtype='float32', name='main_input')\n",
    "    \n",
    "    X = main_input\n",
    "    for nod in n_nod:\n",
    "        X = Dense(nod, \n",
    "                  activation = act_fun,\n",
    "                  kernel_regularizer=regularizers.l2(lambd))(X)\n",
    "        \n",
    "    output = Dense(num_out, activation = out_act_fun )(X)\n",
    "    \n",
    "    method = getattr(optimizers, opt_method)\n",
    "    \n",
    "    model =  Model(inputs=[main_input], outputs=[output])\n",
    "    model.compile(optimizer = method(lr = lr_rate, clipnorm = 1.0),\n",
    "                  loss = cost_fun,\n",
    "                  metrics=['accuracy', 'mse'])   \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_class(x = None, y = None, label = 'Training', modl = None):\n",
    "    \"\"\"\n",
    "    input :  \n",
    "             x = input\n",
    "             y = output\n",
    "             label = \"Provided text string\"\n",
    "             modl = the model\n",
    "             \n",
    "    output : \n",
    "             sensitivity = fraction of correctly classified positive cases\n",
    "             specificity = fraction of correctly classified negative cases\n",
    "             accuracy = fraction of correctly classified cases\n",
    "             loss = typically the cross-entropy error\n",
    "    \"\"\"\n",
    "    \n",
    "    def binary(y1):\n",
    "        y1[y1>.5] = 1.\n",
    "        y1[y1<= .5] = 0.        \n",
    "        return y1\n",
    "\n",
    "    y_pr = modl.predict(x, batch_size = x.shape[0], verbose=0).reshape(y.shape)\n",
    "                \n",
    "    nof_p, tp, nof_n, tn = [np.count_nonzero(k) for k in [y==1, y_pr[y==1.] > 0.5, y==0, y_pr[y==0.]<= 0.5]]\n",
    "    \n",
    "    sens = tp / nof_p\n",
    "    spec = tn / nof_n\n",
    "    acc = (tp + tn) / (len(y))\n",
    "    loss = modl.evaluate(x, y , batch_size =  x.shape[0], verbose=0)\n",
    "                \n",
    "    A = ['Accuracy', 'Sensitivity', 'Specificity', 'Loss']\n",
    "    B = [acc, sens, spec, loss[0]]\n",
    "    \n",
    "    print('\\n','#'*10,'STATISTICS for {} Data'.format(label), '#'*10, '\\n')\n",
    "    for r in zip(A,B):\n",
    "         print(*r, sep = '   ')\n",
    "    return print('\\n','#'*50), acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing our data into sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89976\n",
      "nan\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('/Users/karltengelin/Documents/import_dataset_new.csv', delimiter = ';')\n",
    "\n",
    "#Here we want to see where our prediction set starts (the set of data containing the pd:s we don't know)\n",
    "count = 0 \n",
    "for i in range(len(dataset)):\n",
    "    if math.isnan(dataset.iloc[i]['default']):\n",
    "        break\n",
    "    count+=1\n",
    "    \n",
    "#Just to make sure we have found the start of our prediction set    \n",
    "print(count)\n",
    "print(dataset.iloc[count]['default'])\n",
    "print(dataset.iloc[count-1]['default'])\n",
    "\n",
    "#Training set and labels\n",
    "#Set\n",
    "t_dat = dataset.loc[1:count-1]\n",
    "x_train = t_dat\n",
    "#Labels\n",
    "train_labels = t_dat['default'].values\n",
    "d_train = train_labels\n",
    "\n",
    "#Prediction set\n",
    "p_dat = dataset.loc[count:len(dataset)]\n",
    "x_pred = p_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing unwanted columns (here we choose ONE of the following cells to run):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to remove as few columns as possible (this cell was used to produce the attached CSV-file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "x_train = x_train.drop(labels=['default', 'uuid', 'merchant_category','merchant_group', 'name_in_email', ], axis = 1)\n",
    "x_train = x_train.values\n",
    "\n",
    "#prediction set\n",
    "x_pred = x_pred.drop(labels=['default', 'uuid', 'merchant_category','merchant_group', 'name_in_email', ], axis = 1)\n",
    "x_pred = x_pred.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing columns that contain more than 50% NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "x_train = x_train.drop(labels=['default', 'uuid', 'merchant_category','merchant_group', 'name_in_email', \n",
    "                               'account_incoming_debt_vs_paid_0_24m', 'account_status', \n",
    "                               'account_worst_status_0_3m', 'account_worst_status_12_24m', \n",
    "                               'account_worst_status_3_6m', 'account_worst_status_6_12m', \n",
    "                               'avg_payment_span_0_3m', 'worst_status_active_inv'], axis = 1)\n",
    "x_train = x_train.values\n",
    "\n",
    "#prediction set\n",
    "x_pred = x_pred.drop(labels=['default', 'uuid', 'merchant_category','merchant_group', 'name_in_email', \n",
    "                               'account_incoming_debt_vs_paid_0_24m', 'account_status', \n",
    "                               'account_worst_status_0_3m', 'account_worst_status_12_24m', \n",
    "                               'account_worst_status_3_6m', 'account_worst_status_6_12m', \n",
    "                               'avg_payment_span_0_3m', 'worst_status_active_inv'], axis = 1)\n",
    "x_pred = x_pred.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing input data to have zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = StandardScaler().fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our MLP model and using K-fold cross validation to ensure generalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold 1 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9856625955518233\n",
      "Sensitivity   0.04013663535439795\n",
      "Specificity   0.9995363757110994\n",
      "Loss   0.05982979014515877\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9865525672371638\n",
      "Sensitivity   0.02564102564102564\n",
      "Specificity   0.9992118004729197\n",
      "Loss   0.05389457568526268\n",
      "\n",
      " ##################################################\n",
      "K-fold 2 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9857243414796795\n",
      "Sensitivity   0.037132987910189985\n",
      "Specificity   0.9994863378393616\n",
      "Loss   0.05922059342265129\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9849966659257613\n",
      "Sensitivity   0.007692307692307693\n",
      "Specificity   0.9993234100135318\n",
      "Loss   0.06265977025032043\n",
      "\n",
      " ##################################################\n",
      "K-fold 3 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9857490398508219\n",
      "Sensitivity   0.04561101549053356\n",
      "Specificity   0.9994361962037211\n",
      "Loss   0.05849757418036461\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9857746165814626\n",
      "Sensitivity   0.031746031746031744\n",
      "Specificity   0.9993237150586114\n",
      "Loss   0.05837005376815796\n",
      "\n",
      " ##################################################\n",
      "K-fold 4 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9856872939229658\n",
      "Sensitivity   0.04188034188034188\n",
      "Specificity   0.9995238512912401\n",
      "Loss   0.058852311223745346\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9862191598132919\n",
      "Sensitivity   0.00847457627118644\n",
      "Specificity   0.9992117117117117\n",
      "Loss   0.058865632861852646\n",
      "\n",
      " ##################################################\n",
      "K-fold 5 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9857366906652506\n",
      "Sensitivity   0.03773584905660377\n",
      "Specificity   0.9995865231609677\n",
      "Loss   0.058635562658309937\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9865525672371638\n",
      "Sensitivity   0.02459016393442623\n",
      "Specificity   0.9997746732762506\n",
      "Loss   0.06005041301250458\n",
      "\n",
      " ##################################################\n",
      "K-fold 6 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9859344513324607\n",
      "Sensitivity   0.05349439171699741\n",
      "Specificity   0.9994738094939801\n",
      "Loss   0.05774030461907387\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9855507391352674\n",
      "Sensitivity   0.023255813953488372\n",
      "Specificity   0.9995489400090212\n",
      "Loss   0.06239326298236847\n",
      "\n",
      " ##################################################\n",
      "K-fold 7 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9856380745387636\n",
      "Sensitivity   0.03832752613240418\n",
      "Specificity   0.9992609294751347\n",
      "Loss   0.06051286682486534\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9844392575302878\n",
      "Sensitivity   0.05714285714285714\n",
      "Specificity   0.9990967596251552\n",
      "Loss   0.0648949146270752\n",
      "\n",
      " ##################################################\n",
      "K-fold 8 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9859591493986021\n",
      "Sensitivity   0.03836094158674804\n",
      "Specificity   0.999574100286856\n",
      "Loss   0.05828280374407768\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9841058130487941\n",
      "Sensitivity   0.02127659574468085\n",
      "Specificity   0.9994354110207768\n",
      "Loss   0.06262862682342529\n",
      "\n",
      " ##################################################\n",
      "K-fold 9 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9854898861419151\n",
      "Sensitivity   0.03189655172413793\n",
      "Specificity   0.9993485178781728\n",
      "Loss   0.059596192091703415\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9854395909747694\n",
      "Sensitivity   0.0234375\n",
      "Specificity   0.9993234863005976\n",
      "Loss   0.059837620705366135\n",
      "\n",
      " ##################################################\n",
      "K-fold 10 of 10\n",
      "\n",
      " ########## STATISTICS for Training Data ########## \n",
      "\n",
      "Accuracy   0.9856627726049051\n",
      "Sensitivity   0.03475238922675934\n",
      "Specificity   0.9993736455084119\n",
      "Loss   0.05886327102780342\n",
      "\n",
      " ##################################################\n",
      "\n",
      " ########## STATISTICS for Validation Data ########## \n",
      "\n",
      "Accuracy   0.9846615538512837\n",
      "Sensitivity   0.043795620437956206\n",
      "Specificity   0.9992099322799097\n",
      "Loss   0.06366623193025589\n",
      "\n",
      " ##################################################\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 38min 17s, sys: 2min 3s, total: 1h 40min 21s\n",
      "Wall time: 32min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#training our model\n",
    "scores_val = []\n",
    "scores_train = []\n",
    "iterat= 0\n",
    "K = 10 #meaning that we test on 0.9 of data and validate on 0.1\n",
    "cv = KFold(n_splits=K, random_state=42, shuffle=False)\n",
    "\n",
    "for train_index, test_index in cv.split(x_train):\n",
    "    print('K-fold ' + str(iterat+1) + ' of ' + str(K))\n",
    "    \n",
    "    x_t, x_val, d_t, d_val = x_train[train_index], x_train[test_index], d_train[train_index], d_train[test_index]\n",
    "\n",
    "    # Defining the network, cost function and minimization method\n",
    "    INPUT = {'inp_dim': x_t.shape[1],         \n",
    "             'n_nod': [30,30,30],                      #number of nodes in hidden layer (chosen arbitrary)\n",
    "             'act_fun': 'tanh',                        #activation functions for the hidden layer\n",
    "             'out_act_fun': 'sigmoid',                 #output activation function\n",
    "             'opt_method': 'Adam',                     #minimization method\n",
    "             'cost_fun': 'binary_crossentropy',        #error function\n",
    "             'lr_rate': 0.0005 ,                       #learningrate, this rate was chosen based on \n",
    "                                                       #training-and validation error plots\n",
    "             'num_out' : 1 }                           \n",
    "\n",
    "    # creating the model\n",
    "    model = pipline(**INPUT)\n",
    "\n",
    "    # Printing a summary of the model\n",
    "    #model.summary()\n",
    "\n",
    "    # Training the model\n",
    "    estimator = model.fit(x_t, d_t,\n",
    "                          epochs = 500,      \n",
    "                          validation_data=(x_val, d_val),\n",
    "                          batch_size = x_t.shape[0],        # Batch size = all data (batch learning)\n",
    "                          #batch_size=50,                   # Batch size for true SGD\n",
    "                          verbose = 0)\n",
    "\n",
    "    # Call the stats function to print out statistics for classification problems\n",
    "    _,acc_t = stats_class(x_t, d_t, 'Training', model)\n",
    "    _,acc_v = stats_class(x_val, d_val, 'Validation', model)\n",
    "    \n",
    "    # Append accuracy scores so we can validate results\n",
    "    scores_val.append(acc_v)\n",
    "    scores_train.append(acc_t)\n",
    "    \n",
    "    iterat+=1\n",
    "    if iterat == 10:\n",
    "        # Plotting the last learning curve as an example\n",
    "        plt.plot(estimator.history['loss'])\n",
    "        plt.plot(estimator.history['val_loss'])\n",
    "        plt.title('Model training')\n",
    "        plt.ylabel('training error')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc=0)\n",
    "        plt.show()\n",
    "        #If we want to save the figure:\n",
    "        #plt.savefig('/Users/karltengelin/Documents/Train_val_error_case_study_klarna.png',format='png')\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation accuracy: 0.9854292531335245\n",
      "Average training accuracy: 0.9857244295487189\n"
     ]
    }
   ],
   "source": [
    "print('Average validation accuracy: ' + str(np.mean(scores_val)))\n",
    "print('Average training accuracy: ' + str(np.mean(scores_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming to zero mean and unit variance\n",
    "x_pred = StandardScaler().fit_transform(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "pred_labels = model.predict(x_pred)\n",
    "\n",
    "#matching our uuid:s to our labels and storing in csv-file\n",
    "uuid = dataset.iloc[count:len(dataset)]['uuid'].values\n",
    "\n",
    "listing = []\n",
    "for i in range(len(uuid)):\n",
    "    listing.append([uuid[i],pred_labels[i].item()])\n",
    "\n",
    "df = pd.DataFrame(listing)\n",
    "df.columns = [\"uuid\", \"pd\"]\n",
    "#csv_data = df.to_csv('/Users/karltengelin/Documents/probability_of_defaults.csv',index=False, sep=';')\n",
    "csv_data = df.to_csv(index=False, sep =';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99895036]\n",
      "[0.00031391]\n"
     ]
    }
   ],
   "source": [
    "print(max(pred_labels))\n",
    "print(min(pred_labels))\n",
    "#we see that we pass the sanity check of having values inbetween 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.53991455]\n",
      "[0.65417755]\n",
      "[0.53218025]\n",
      "[0.60405356]\n",
      "[0.61388874]\n",
      "[0.67580533]\n",
      "[0.86604416]\n",
      "[0.9986354]\n",
      "[0.76589715]\n",
      "[0.74693835]\n",
      "[0.5728623]\n",
      "[0.75853]\n",
      "[0.5698374]\n",
      "[0.7698958]\n",
      "[0.59286046]\n",
      "[0.99895036]\n",
      "[0.75551945]\n",
      "Ratio of defaulted uuid:s in prediction set: 0.0017\n",
      "Ratio of defaulted uuid:s in test set: 0.014314928425357873\n"
     ]
    }
   ],
   "source": [
    "#Checking how many of our uuid:s that have a higher than 50% probability of default\n",
    "d50 = 0\n",
    "for i in pred_labels:\n",
    "    if i > 0.50:\n",
    "        print(i)\n",
    "        d50 +=1\n",
    "print('Ratio of defaulted uuid:s in prediction set: ' + str(d50/10000))\n",
    "print('Ratio of defaulted uuid:s in test set: ' + str(1288/89976))\n",
    "#These results are a bit worrying since a much smaller portion of the prediction set are thought to default, \n",
    "# this might be an indication that we need to create a one hot encoding for the \n",
    "# merchant_cathegory and merchant_group columns. If we lower the threshold to assuming that uuid:s with a pd > 15% \n",
    "# will default we get approximately the same default ratio (1.4-1.5%) as for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comments:\n",
    "# I made a try without columns that contained to many missing values (>40%) and got the following \n",
    "# min_pred_labels and max_pred_labels: max[0.9990], min[0.0003]\n",
    "# The model seemed to produce similar results when not including low-quality columns as when they were included\n",
    "# hence we might as well keep them in the model.\n",
    "\n",
    "# One could try to use PCA to do a predictive model instead to see if we get different results \n",
    "# that could prove more realistic.\n",
    "# This would be a way of simplifying the problem, but since we got high train and \n",
    "# validation scores for our full dataset I opted to use the full dataset.\n",
    "# If one would want to use PCA the script following header 'PCA:' describes how to do PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ____________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.46166596e-01, -4.12499492e-02, -2.35266618e-01, ...,\n",
       "         1.09644057e-01, -4.28380119e-01, -1.19433870e-03],\n",
       "       [-3.46166596e-01, -4.12499492e-02, -2.35266618e-01, ...,\n",
       "         9.57808894e-01, -7.50296435e-01, -6.40949743e-01],\n",
       "       [-3.46166596e-01,  9.51595938e-04, -1.19735218e-03, ...,\n",
       "         3.19314970e+00,  8.24342626e-02, -6.40949743e-01],\n",
       "       ...,\n",
       "       [-3.46166596e-01, -4.12499492e-02, -2.35266618e-01, ...,\n",
       "         9.05386760e-02,  6.65031013e-01, -1.19433870e-03],\n",
       "       [-3.46166596e-01, -4.12499492e-02, -2.35266618e-01, ...,\n",
       "        -2.87273596e-01, -6.70286702e-01, -1.19433870e-03],\n",
       "       [-3.46166596e-01,  9.51595938e-04, -1.19735218e-03, ...,\n",
       "        -3.44544997e-01,  7.13511721e-01, -1.19433870e-03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.drop(labels=['default', 'uuid', 'merchant_category','merchant_group', 'name_in_email', ], axis = 1)\n",
    "x_train = StandardScaler().fit_transform(x_train)\n",
    "\n",
    "#Extracting the 5 most principal components\n",
    "pca = PCA(n_components=5)\n",
    "principalComponents = pca.fit_transform(x_train)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2', \n",
    "                          'principal component 3', 'principal component 4', \n",
    "                          'principal component 5'])\n",
    "print(principalDf)\n",
    "# We can then use principalDf the same way as we used training set in our model above\n",
    "# (and also we would need to do PCA on prediction set aswell)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
